{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of FAERS_download_datamanagement_part1_final1.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LwbkF1GMdSo"
      },
      "source": [
        "# FAERS Public dataset download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTIxUDKMdSy"
      },
      "source": [
        "### Importing all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36dTcFUpMdS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676e1805-8fea-42b9-b22b-2ba94cbac2d4"
      },
      "source": [
        "!pip install geopandas\n",
        "import geopandas as gpd\n",
        "\n",
        "import pandas as pd\n",
        "#import vaex as vd\n",
        "import numpy as np\n",
        "from shapely.geometry import Point\n",
        "\n",
        "#For data handling and munging. This is an extremely powerful tool\n",
        "#for working with data in a spreadsheet-like format. If you’re familiar with R data.frames, then you’ll love pandas.\n",
        "import fiona\n",
        "#fiona - For making it easy to read/write geospatial data formats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "#for plotting\n",
        "\n",
        "import os\n",
        "#sys library\n",
        "\n",
        "from functools import reduce"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geopandas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/bf/e9cefb69d39155d122b6ddca53893b61535fa6ffdad70bf5ef708977f53f/geopandas-0.9.0-py2.py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.5MB/s \n",
            "\u001b[?25hCollecting fiona>=1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/2a/404b22883298a3efe9c6ef8d67acbf2c38443fa366ee9cd4cd34e17626ea/Fiona-1.8.19-cp37-cp37m-manylinux1_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Collecting pyproj>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/72/d52e9ca81caef056062d71991b0e9b1d16af042245627c5d0e4916a36c4f/pyproj-3.0.1-cp37-cp37m-manylinux2010_x86_64.whl (6.5MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2020.12.5)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1e/947eadf10d6804bf276eb8a038bd5307996dceaaa41cfd21b7a15ec62f5d/cligj-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (1.19.5)\n",
            "Installing collected packages: click-plugins, munch, cligj, fiona, pyproj, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.1 fiona-1.8.19 geopandas-0.9.0 munch-2.5.0 pyproj-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4SObKK2MdS3"
      },
      "source": [
        "#importing\n",
        "#requesting files over the internet\n",
        "#import requests\n",
        "import urllib.request\n",
        "\n",
        "#unpacking zipfiles\n",
        "import zipfile\n",
        "import tarfile\n",
        "import urllib.request, shutil\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BI5L0qiNiQn"
      },
      "source": [
        "###mounting drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut_EGOgENi20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "accd5161-3015-4e1e-b588-28cdfee7741f"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcbGnE-kUl39"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/colab_faers_data/Data/zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAHnp3LuMdS6"
      },
      "source": [
        "## Importing latest FAERS dataset from openFDA api\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m--N-9QYMdS7"
      },
      "source": [
        "#### brute force download (use 1 only)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDFIqsfMMdS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f445ed6-223d-463f-deda-bc97146a0ea2"
      },
      "source": [
        "#brute force download\n",
        "for i in range(2013,2021):\n",
        "    for j in range (1,5):\n",
        "        url = \"https://fis.fda.gov/content/Exports/faers_ascii_\"+str(i)+\"q\"+str(j)+\".zip\"#2013 and up\n",
        "        #url = \"https://fis.fda.gov/content/Exports/aers_ascii_\"+str(i)+\"q\"+str(j)+\".zip\" #earlier database\n",
        "\n",
        "        #print(i,\"_\",j)\n",
        "        print(url)\n",
        "        file_name = 'FAERS_'+str(i)+'q'+str(j)+'.zip'\n",
        "\n",
        "        with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "            shutil.copyfileobj(response, out_file)\n",
        "print('zipfile download complete')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://fis.fda.gov/content/Exports/faers_ascii_2013q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2013q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2013q3.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2013q4.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2014q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2014q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2014q3.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2014q4.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2015q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2015q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2015q3.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2015q4.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2016q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2016q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2016q3.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2016q4.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2017q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2017q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2017q3.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2017q4.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2018q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2018q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2018q3.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2018q4.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2019q1.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2019q2.zip\n",
            "https://fis.fda.gov/content/Exports/faers_ascii_2019q3.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwKjPVdsMdS-"
      },
      "source": [
        "#### piece meal download (use 1 only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTJTNxBMdS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cba3c09-3ef6-4196-e23c-474b7c1c3f2c"
      },
      "source": [
        "###use one or the other\n",
        "###what year do you want to download\n",
        "###data starts from year 2004\n",
        "i= 2020\n",
        "###what quarter do you want to download (4 options q1, q2, q3, q4)\n",
        "j = 1#quarter\n",
        "url = \"https://fis.fda.gov/content/Exports/faers_ascii_\"+str(i)+\"q\"+str(j)+\".zip\"#2013 and up\n",
        "#url_ = \"https://fis.fda.gov/content/Exports/aers_ascii_\"+str(i_)+\"q\"+str(j_)+\".zip\" #earlier databaseprint(url_)\n",
        "file_name = 'FAERS_'+str(i)+'q'+str(j)+'.zip'\n",
        "#file_name = 'FLOODOUTLOOK_LATEST_simp_wgs84.zip'\n",
        "\n",
        "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "  shutil.copyfileobj(response, out_file)\n",
        "\n",
        "print('zipfile download complete')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zipfile download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyYAkabXMdTD"
      },
      "source": [
        "### unzip  dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAQhh6eGMdTF"
      },
      "source": [
        "#### one folder at a time "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgZuXq0rMdTG",
        "outputId": "6f8a0948-6c96-4b73-e169-2339196ace4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "i_= 2014 #year\n",
        "###what quarter do you want to download (4 options q1, q2, q3, q4)\n",
        "j_ =4 #quarter\n",
        "file_name = 'FAERS_'+str(i_)+'q'+str(j_)+'.zip'\n",
        "print(file_name)\n",
        "\n",
        "zip_path='/content/drive/MyDrive/colab_faers_data/Data/zip'\n",
        "unzip_path='/content/drive/MyDrive/colab_faers_data/Data/unzip'\n",
        "os.chdir(zip_path)\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(file_name)\n",
        "#zip_ref.printdir() \n",
        "os.chdir(unzip_path)\n",
        "zip_ref.extractall('FAERS_'+str(i_)+'q'+str(j_))\n",
        "zip_ref.close()\n",
        "print('zipfile unzip complete')\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAERS_2014q4.zip\n",
            "zipfile unzip complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_kH3-SeMdTI"
      },
      "source": [
        "#### batch unzip folders "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuNGbSZgMdTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6273ef67-7305-4805-8725-064273c01763"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "for i_ in range(2013, 2015):\n",
        "    for j_ in range (1,5):\n",
        "        zip_path='/content/drive/MyDrive/colab_faers_data/Data/zip'\n",
        "        unzip_path='/content/drive/MyDrive/colab_faers_data/Data/unzip'\n",
        "\n",
        "        #zip_path = r\"D:\\Research\\FAERS_Public\\Data_20210222\\Orignal_data\\zipfile_data\"\n",
        "       # unzip_path = r\"D:\\Research\\FAERS_Public\\Data_20210222\\Orignal_data\\unzipped_data\"\n",
        "        os.chdir(zip_path)\n",
        "        file_name = 'FAERS_'+str(i_)+'q'+str(j_)+'.zip'\n",
        "        print(file_name)\n",
        "        zip_ref = zipfile.ZipFile(file_name)\n",
        "#zip_ref.printdir() \n",
        "        os.chdir(unzip_path)\n",
        "        zip_ref.extractall('FAERS_'+str(i_)+'q'+str(j_))\n",
        "        zip_ref.close()\n",
        "        print('zipfile unzip complete')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAERS_2013q1.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2013q2.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2013q3.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2013q4.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2014q1.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2014q2.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2014q3.zip\n",
            "zipfile unzip complete\n",
            "FAERS_2014q4.zip\n",
            "zipfile unzip complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAkrdbMegPpj"
      },
      "source": [
        "#####change files names here\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3UOcFEtgPTI"
      },
      "source": [
        "#os.chdir('/content/drive/MyDrive/colab_faers_data/Data/zip')\n",
        " \n",
        "os.rename(\"/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_2018q1/ascii/DEMO18Q1_new.txt\",\n",
        "          \"/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_2018q1/ascii/DEMO18Q1.txt\")\n",
        "os.rename(\"/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_2013q3/ASCII\",\n",
        "\"/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_2013q3/ascii\")\n",
        "for i_ in range(2020, 2021):\n",
        "    for j_ in range (1,5):\n",
        "        os.rename(\"/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_\"+str(i_)+\"q\"+str(j_)+\"/ASCII\",\n",
        "          \"/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_\"+str(i_)+\"q\"+str(j_)+\"/ascii\")\n",
        "        print(i_,j_)\n",
        "\n",
        "print(\"The File has been successfully renamed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lerpIYVnssEH"
      },
      "source": [
        "#####change column names here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IbmxNUjuMo5",
        "outputId": "91e98926-95b3-417b-b249-d607af29e4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "file = pd.read_csv('/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_2014q1/ascii/DEMO14Q1.txt',delimiter=\"$\",error_bad_lines=False, encoding='utf-8')\n",
        "#file.rename(columns = {'gndr_cod': 'sex'}, inplace = True)\n",
        "#file.to_csv('/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_2014q1/ascii/DEMO14Q1.txt',sep=\"$\")\n",
        "\n",
        "file.head()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>primaryid</th>\n",
              "      <th>caseid</th>\n",
              "      <th>caseversion</th>\n",
              "      <th>i_f_code</th>\n",
              "      <th>event_dt</th>\n",
              "      <th>mfr_dt</th>\n",
              "      <th>init_fda_dt</th>\n",
              "      <th>fda_dt</th>\n",
              "      <th>rept_cod</th>\n",
              "      <th>mfr_num</th>\n",
              "      <th>mfr_sndr</th>\n",
              "      <th>age</th>\n",
              "      <th>age_cod</th>\n",
              "      <th>sex</th>\n",
              "      <th>e_sub</th>\n",
              "      <th>wt</th>\n",
              "      <th>wt_cod</th>\n",
              "      <th>rept_dt</th>\n",
              "      <th>to_mfr</th>\n",
              "      <th>occp_cod</th>\n",
              "      <th>reporter_country</th>\n",
              "      <th>occr_country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100033001</td>\n",
              "      <td>10003300</td>\n",
              "      <td>1</td>\n",
              "      <td>I</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20131015.0</td>\n",
              "      <td>20140306</td>\n",
              "      <td>20140306</td>\n",
              "      <td>PER</td>\n",
              "      <td>1289378</td>\n",
              "      <td>GENENTECH</td>\n",
              "      <td>77.0</td>\n",
              "      <td>YR</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140124.0</td>\n",
              "      <td>N</td>\n",
              "      <td>CN</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100033011</td>\n",
              "      <td>10003301</td>\n",
              "      <td>1</td>\n",
              "      <td>I</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20130729.0</td>\n",
              "      <td>20140228</td>\n",
              "      <td>20140228</td>\n",
              "      <td>PER</td>\n",
              "      <td>US-JNJFOC-20130719067</td>\n",
              "      <td>JANSSEN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140109.0</td>\n",
              "      <td>N</td>\n",
              "      <td>CN</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100033021</td>\n",
              "      <td>10003302</td>\n",
              "      <td>1</td>\n",
              "      <td>I</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140307.0</td>\n",
              "      <td>20140312</td>\n",
              "      <td>20140312</td>\n",
              "      <td>PER</td>\n",
              "      <td>US-PFIZER INC-2014068976</td>\n",
              "      <td>PFIZER</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140311.0</td>\n",
              "      <td>N</td>\n",
              "      <td>CN</td>\n",
              "      <td>US</td>\n",
              "      <td>US</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100033051</td>\n",
              "      <td>10003305</td>\n",
              "      <td>1</td>\n",
              "      <td>I</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140307.0</td>\n",
              "      <td>20140312</td>\n",
              "      <td>20140312</td>\n",
              "      <td>PER</td>\n",
              "      <td>US-PFIZER INC-2014069067</td>\n",
              "      <td>PFIZER</td>\n",
              "      <td>48.0</td>\n",
              "      <td>YR</td>\n",
              "      <td>F</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140312.0</td>\n",
              "      <td>N</td>\n",
              "      <td>MD</td>\n",
              "      <td>US</td>\n",
              "      <td>US</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100033061</td>\n",
              "      <td>10003306</td>\n",
              "      <td>1</td>\n",
              "      <td>I</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20120731.0</td>\n",
              "      <td>20140312</td>\n",
              "      <td>20140312</td>\n",
              "      <td>EXP</td>\n",
              "      <td>US-GILEAD-2012-0059171</td>\n",
              "      <td>GILEAD</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20140312.0</td>\n",
              "      <td>N</td>\n",
              "      <td>CN</td>\n",
              "      <td>US</td>\n",
              "      <td>US</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   primaryid    caseid  caseversion  ... occp_cod  reporter_country  occr_country\n",
              "0  100033001  10003300            1  ...       CN                US           NaN\n",
              "1  100033011  10003301            1  ...       CN                US           NaN\n",
              "2  100033021  10003302            1  ...       CN                US            US\n",
              "3  100033051  10003305            1  ...       MD                US            US\n",
              "4  100033061  10003306            1  ...       CN                US            US\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4wWQHrWi1pv"
      },
      "source": [
        "###Reading files \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndnldy9me4Sc"
      },
      "source": [
        "after **2018** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1YmzQioj0WI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1753040a-8444-439e-f51a-43bd46b7790b"
      },
      "source": [
        "df1_appended_data0 = []\n",
        "df2_appended_data0 = []\n",
        "df3_appended_data0 = []\n",
        "#df4_appended_data0 = []\n",
        "df5_appended_data0 = []\n",
        "#df6_appended_data0 = []\n",
        "#df7_appended_data0 = []\n",
        "chunk_size=5000\n",
        "for i in range(2020,2021):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "      \n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,delimiter=\"$\",error_bad_lines=False, encoding=\"cp1252\", usecols=usecols2)\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\", usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "       # print(df2.columns)\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "\n",
        "        df1_appended_data0.append(df1)\n",
        "        df2_appended_data0.append(df2)\n",
        "        df3_appended_data0.append(df3)\n",
        "        #df4_appended_data0.append(df4)\n",
        "        df5_appended_data0.append(df5)\n",
        "        #df6_appended_data0.append(df6)\n",
        "        #df7_appended_data0.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data0 = pd.concat(df1_appended_data0, axis=0)\n",
        "df2_appended_data0 = pd.concat(df2_appended_data0, axis=0)\n",
        "df3_appended_data0 = pd.concat(df3_appended_data0, axis=0)\n",
        "#df4_appended_data0 = pd.concat(df4_appended_data0, axis=0)\n",
        "df5_appended_data0 = pd.concat(df5_appended_data0, axis=0)\n",
        "#df6_appended_data0 = pd.concat(df6_appended_data0, axis=0)\n",
        "#df7_appended_data0 = pd.concat(df7_appended_data0, axis=0)\n",
        "print('done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "1\n",
            "20\n",
            "2\n",
            "20\n",
            "3\n",
            "20\n",
            "4\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6WoG5Wgiymd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584e8629-065f-427f-801e-4e18184665f6"
      },
      "source": [
        "df1_appended_data0b = []\n",
        "df2_appended_data0b = []\n",
        "df3_appended_data0b = []\n",
        "#df4_appended_data0b = []\n",
        "df5_appended_data0b = []\n",
        "#df6_appended_data0b = []\n",
        "#df7_appended_data0b = []\n",
        "\n",
        "for i in range(2019,2020):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "      \n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,delimiter=\"$\",error_bad_lines=False, encoding=\"cp1252\", usecols=usecols2)\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\", usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "       # print(df2.columns)\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "\n",
        "        df1_appended_data0b.append(df1)\n",
        "        df2_appended_data0b.append(df2)\n",
        "        df3_appended_data0b.append(df3)\n",
        "        #df4_appended_data0b.append(df4)\n",
        "        df5_appended_data0b.append(df5)\n",
        "        #df6_appended_data0b.append(df6)\n",
        "        #df7_appended_data0b.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data0b = pd.concat(df1_appended_data0b, axis=0)\n",
        "df2_appended_data0b = pd.concat(df2_appended_data0b, axis=0)\n",
        "df3_appended_data0b = pd.concat(df3_appended_data0b, axis=0)\n",
        "#df4_appended_data0b = pd.concat(df4_appended_data0b, axis=0)\n",
        "df5_appended_data0b = pd.concat(df5_appended_data0b, axis=0)\n",
        "#df6_appended_data0b = pd.concat(df6_appended_data0b, axis=0)\n",
        "#df7_appended_data0b = pd.concat(df7_appended_data0b, axis=0)\n",
        "print('done')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n",
            "1\n",
            "19\n",
            "2\n",
            "19\n",
            "3\n",
            "19\n",
            "4\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJqXfhcMYyiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95a885d-570b-44ad-d46c-f75468f9ce2e"
      },
      "source": [
        "df1_appended_data0c = []\n",
        "df2_appended_data0c = []\n",
        "df3_appended_data0c = []\n",
        "#df4_appended_data0c = []\n",
        "df5_appended_data0c = []\n",
        "#df6_appended_data0c = []\n",
        "#df7_appended_data0c = []\n",
        "\n",
        "for i in range(2018,2019):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "      \n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,delimiter=\"$\",error_bad_lines=False, encoding=\"cp1252\", usecols=usecols2)\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\", usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "       # print(df2.columns)\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "\n",
        "        df1_appended_data0c.append(df1)\n",
        "        df2_appended_data0c.append(df2)\n",
        "        df3_appended_data0c.append(df3)\n",
        "        #df4_appended_data0c.append(df4)\n",
        "        df5_appended_data0c.append(df5)\n",
        "        #df6_appended_data0c.append(df6)\n",
        "        #df7_appended_data0c.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data0c = pd.concat(df1_appended_data0c, axis=0)\n",
        "df2_appended_data0c = pd.concat(df2_appended_data0c, axis=0)\n",
        "df3_appended_data0c = pd.concat(df3_appended_data0c, axis=0)\n",
        "#df4_appended_data0c = pd.concat(df4_appended_data0c, axis=0)\n",
        "df5_appended_data0c = pd.concat(df5_appended_data0c, axis=0)\n",
        "#df6_appended_data0c = pd.concat(df6_appended_data0c, axis=0)\n",
        "#df7_appended_data0c = pd.concat(df7_appended_data0c, axis=0)\n",
        "print('done')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n",
            "1\n",
            "18\n",
            "2\n",
            "18\n",
            "3\n",
            "18\n",
            "4\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHYJbvriejnE"
      },
      "source": [
        "2015 to **2017**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c41anGkEMdTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a42685c-207a-4939-c953-92f506988a55"
      },
      "source": [
        "df1_appended_data1a = []\n",
        "df2_appended_data1a = []\n",
        "df3_appended_data1a = []\n",
        "#df4_appended_data1a = []\n",
        "df5_appended_data1a = []\n",
        "#df6_appended_data1a = []\n",
        "#df7_appended_data1a = []\n",
        "\n",
        "for i in range(2017,2018):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,delimiter=\"$\",error_bad_lines=False, encoding=\"cp1252\", usecols=usecols2)\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\", usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "       # print(df2.columns)\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "\n",
        "        df1_appended_data1a.append(df1)\n",
        "        df2_appended_data1a.append(df2)\n",
        "        df3_appended_data1a.append(df3)\n",
        "        #df4_appended_data1a.append(df4)\n",
        "        df5_appended_data1a.append(df5)\n",
        "        #df6_appended_data1a.append(df6)\n",
        "        #df7_appended_data1a.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data1a = pd.concat(df1_appended_data1a, axis=0)\n",
        "df2_appended_data1a = pd.concat(df2_appended_data1a, axis=0)\n",
        "df3_appended_data1a = pd.concat(df3_appended_data1a, axis=0)\n",
        "#df4_appended_data1a = pd.concat(df4_appended_data1a, axis=0)\n",
        "df5_appended_data1a = pd.concat(df5_appended_data1a, axis=0)\n",
        "#df6_appended_data1a = pd.concat(df6_appended_data1a, axis=0)\n",
        "#df7_appended_data1a = pd.concat(df7_appended_data1a, axis=0)\n",
        "print('done')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "1\n",
            "17\n",
            "2\n",
            "17\n",
            "3\n",
            "17\n",
            "4\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FVkn__3m4Y6",
        "outputId": "fc518fc0-b176-4431-aac9-34e497185c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1_appended_data1b = []\n",
        "df2_appended_data1b = []\n",
        "df3_appended_data1b = []\n",
        "#df4_appended_data1b = []\n",
        "df5_appended_data1b = []\n",
        "#df6_appended_data1b = []\n",
        "#df7_appended_data1b = []\n",
        "\n",
        "for i in range(2016,2017):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,delimiter=\"$\",error_bad_lines=False, encoding=\"cp1252\", usecols=usecols2)\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\", usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "       # print(df2.columns)\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "\n",
        "        df1_appended_data1b.append(df1)\n",
        "        df2_appended_data1b.append(df2)\n",
        "        df3_appended_data1b.append(df3)\n",
        "        #df4_appended_data1b.append(df4)\n",
        "        df5_appended_data1b.append(df5)\n",
        "        #df6_appended_data1b.append(df6)\n",
        "        #df7_appended_data1b.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data1b = pd.concat(df1_appended_data1b, axis=0)\n",
        "df2_appended_data1b = pd.concat(df2_appended_data1b, axis=0)\n",
        "df3_appended_data1b = pd.concat(df3_appended_data1b, axis=0)\n",
        "#df4_appended_data1b = pd.concat(df4_appended_data1b, axis=0)\n",
        "df5_appended_data1b = pd.concat(df5_appended_data1b, axis=0)\n",
        "#df6_appended_data1b = pd.concat(df6_appended_data1b, axis=0)\n",
        "#df7_appended_data1b = pd.concat(df7_appended_data1b, axis=0)\n",
        "print('done')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "1\n",
            "16\n",
            "2\n",
            "16\n",
            "3\n",
            "16\n",
            "4\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cwX9_d2m42k",
        "outputId": "ceac183d-a4ba-4a8e-ea27-17129839ddd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1_appended_data1c = []\n",
        "df2_appended_data1c = []\n",
        "df3_appended_data1c = []\n",
        "#df4_appended_data1c = []\n",
        "df5_appended_data1c = []\n",
        "#df6_appended_data1c = []\n",
        "#df7_appended_data1c = []\n",
        "\n",
        "for i in range(2015,2016):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,delimiter=\"$\",error_bad_lines=False, encoding=\"cp1252\", usecols=usecols2)\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\", usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,delimiter=\"$\",error_bad_lines=False, encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "       # print(df2.columns)\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "\n",
        "        df1_appended_data1c.append(df1)\n",
        "        df2_appended_data1c.append(df2)\n",
        "        df3_appended_data1c.append(df3)\n",
        "        #df4_appended_data1c.append(df4)\n",
        "        df5_appended_data1c.append(df5)\n",
        "        #df6_appended_data1c.append(df6)\n",
        "        #df7_appended_data1c.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data1c = pd.concat(df1_appended_data1c, axis=0)\n",
        "df2_appended_data1c = pd.concat(df2_appended_data1c, axis=0)\n",
        "df3_appended_data1c = pd.concat(df3_appended_data1c, axis=0)\n",
        "#df4_appended_data1c = pd.concat(df4_appended_data1c, axis=0)\n",
        "df5_appended_data1c = pd.concat(df5_appended_data1c, axis=0)\n",
        "#df6_appended_data1c = pd.concat(df6_appended_data1c, axis=0)\n",
        "#df7_appended_data1c = pd.concat(df7_appended_data1c, axis=0)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "1\n",
            "15\n",
            "2\n",
            "15\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1DzUIneMdTQ"
      },
      "source": [
        "### FAERS dataset year 2013, 2014\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sycs8ENIMdTQ"
      },
      "source": [
        "df1_appended_data2a = []\n",
        "df2_appended_data2a = []\n",
        "df3_appended_data2a = []\n",
        "#df4_appended_data2a = []\n",
        "df5_appended_data2a = []\n",
        "#df6_appended_data2a = []\n",
        "#df7_appended_data2a = []\n",
        "\n",
        "for i in range(2014,2015):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,error_bad_lines=False,delimiter=\"$\" ,encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,error_bad_lines=False,delimiter=\"$\" ,encoding=\"cp1252\")\n",
        "        df2 =df2[df2.columns[[0,1,4,-1]]]\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\" ,usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,error_bad_lines=False,delimiter=\"$\" ,encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "        df1_appended_data2a.append(df1)\n",
        "        df2_appended_data2a.append(df2)\n",
        "        df3_appended_data2a.append(df3)\n",
        "      #  df4_appended_data2a.append(df4)\n",
        "        df5_appended_data2a.append(df5)\n",
        "      #  df6_appended_data2a.append(df6)\n",
        "       # df7_appended_data2a.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data2a = pd.concat(df1_appended_data2a, axis=0)\n",
        "df2_appended_data2a = pd.concat(df2_appended_data2a, axis=0)\n",
        "\n",
        "df3_appended_data2a = pd.concat(df3_appended_data2a, axis=0)\n",
        "#df4_appended_data2a = pd.concat(df4_appended_data2a, axis=0)\n",
        "df5_appended_data2a = pd.concat(df5_appended_data2a, axis=0)\n",
        "#df6_appended_data2a = pd.concat(df6_appended_data2a, axis=0)\n",
        "#df7_appended_data2a = pd.concat(df7_appended_data2a, axis=0)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyYJR7ljnzf8"
      },
      "source": [
        "df1_appended_data2b = []\n",
        "df2_appended_data2b = []\n",
        "df3_appended_data2b = []\n",
        "#df4_appended_data2b = []\n",
        "df5_appended_data2b = []\n",
        "#df6_appended_data2b = []\n",
        "#df7_appended_data2b = []\n",
        "\n",
        "for i in range(2013,2014):\n",
        "    for j in range (1,5):\n",
        "        ii = str(i)\n",
        "        ii =(ii[-2:])\n",
        "        print(ii)\n",
        "        print(j)\n",
        "        df1_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DEMO'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df2_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'DRUG'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df3_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'INDI'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df4_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'OUTC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        df5_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'REAC'+str(ii)+'Q'+str(j)+'.txt'\n",
        "\n",
        "        #df6_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'RPSR'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        #df7_filepath =  '/content/drive/MyDrive/colab_faers_data/Data/unzip/FAERS_'+str(i)+'q'+str(j)+'/'+'ascii'+'/'+'THER'+str(ii)+'Q'+str(j)+'.txt'\n",
        "        usecols1=[\"primaryid\",\"caseid\", \"event_dt\",\"occr_country\", 'age', 'sex' ]\n",
        "        usecols2=[\"primaryid\",\"caseid\", \"drugname\",\"prod_ai\"]\n",
        "        usecols3=[\"primaryid\",\"caseid\", \"indi_pt\"]\n",
        "        usecols5=[\"primaryid\",\"caseid\", \"pt\"]\n",
        "\n",
        "\n",
        "        df1 = pd.read_csv(df1_filepath,error_bad_lines=False,delimiter=\"$\" ,encoding='utf-8', usecols=usecols1)\n",
        "        df2 = pd.read_csv(df2_filepath,error_bad_lines=False,delimiter=\"$\" ,encoding=\"cp1252\")\n",
        "        df2 =df2[df2.columns[[0,1,4,-1]]]\n",
        "        df3 = pd.read_csv(df3_filepath,delimiter=\"$\" ,usecols=usecols3)\n",
        "        #df4 = pd.read_csv(df4_filepath,delimiter=\"$\")\n",
        "        df5 = pd.read_csv(df5_filepath,error_bad_lines=False,delimiter=\"$\" ,encoding='utf-8', usecols=usecols5)\n",
        "        #df6 = pd.read_csv(df6_filepath,delimiter=\"$\")\n",
        "        #df7 = pd.read_csv(df7_filepath,delimiter=\"$\")\n",
        "\n",
        "        df1.columns = df1.columns.str.lower()  \n",
        "        df2.columns = df2.columns.str.lower()  \n",
        "        df3.columns = df3.columns.str.lower()  \n",
        "        #df4.columns = df4.columns.str.lower()  \n",
        "        df5.columns = df5.columns.str.lower() \n",
        "        #df6.columns = df6.columns.str.lower()  \n",
        "        #df7.columns = df7.columns.str.lower() \n",
        "\n",
        "        df1_appended_data2b.append(df1)\n",
        "        df2_appended_data2b.append(df2)\n",
        "        df3_appended_data2b.append(df3)\n",
        "      #  df4_appended_data2b.append(df4)\n",
        "        df5_appended_data2b.append(df5)\n",
        "      #  df6_appended_data2b.append(df6)\n",
        "       # df7_appended_data2b.append(df7)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "df1_appended_data2b = pd.concat(df1_appended_data2b, axis=0)\n",
        "df2_appended_data2b = pd.concat(df2_appended_data2b, axis=0)\n",
        "\n",
        "df3_appended_data2b = pd.concat(df3_appended_data2b, axis=0)\n",
        "#df4_appended_data2b = pd.concat(df4_appended_data2b, axis=0)\n",
        "df5_appended_data2b = pd.concat(df5_appended_data2b, axis=0)\n",
        "#df6_appended_data2b = pd.concat(df6_appended_data2b, axis=0)\n",
        "#df7_appended_data2b = pd.concat(df7_appended_data2b, axis=0)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMBqj8goMdTU"
      },
      "source": [
        "df1_appended_data0['primaryid'] = df1_appended_data0['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df1_appended_data0['caseid'] = df1_appended_data0['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df1_appended_data1['primaryid'] = df1_appended_data1['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df1_appended_data1['caseid'] = df1_appended_data1['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df1_appended_data2['primaryid'] = df1_appended_data2['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df1_appended_data2['caseid'] = df1_appended_data2['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df1_appended_data0['primaryid'] = df1_appended_data0['primaryid'].astype(str)\n",
        "df1_appended_data0['caseid'] = df1_appended_data0['caseid'].astype(str)\n",
        "\n",
        "df1_appended_data1['primaryid'] = df1_appended_data1['primaryid'].astype(str)\n",
        "df1_appended_data1['caseid'] = df1_appended_data1['caseid'].astype(str)\n",
        "\n",
        "df1_appended_data2['primaryid'] = df1_appended_data2['primaryid'].astype(str)\n",
        "df1_appended_data2['caseid'] = df1_appended_data2['caseid'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAvkav0bMdTU"
      },
      "source": [
        "df2_appended_data0['primaryid'] = df2_appended_data0['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df2_appended_data0['caseid'] = df2_appended_data0['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df2_appended_data1['primaryid'] = df2_appended_data1['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df2_appended_data1['caseid'] = df2_appended_data1['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df2_appended_data2['primaryid'] = df2_appended_data2['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df2_appended_data2['caseid'] = df2_appended_data2['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df2_appended_data0['primaryid'] = df2_appended_data0['primaryid'].astype(str)\n",
        "df2_appended_data0['caseid'] = df2_appended_data0['caseid'].astype(str)\n",
        "\n",
        "df2_appended_data1['primaryid'] = df2_appended_data1['primaryid'].astype(str)\n",
        "df2_appended_data1['caseid'] = df2_appended_data1['caseid'].astype(str)\n",
        "\n",
        "df2_appended_data2['primaryid'] = df2_appended_data2['primaryid'].astype(str)\n",
        "df2_appended_data2['caseid'] = df2_appended_data2['caseid'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6HAR4W6MdTU"
      },
      "source": [
        "df5_appended_data0['primaryid'] = df5_appended_data0['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df5_appended_data0['caseid'] = df5_appended_data0['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df5_appended_data1['primaryid'] = df5_appended_data1['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df5_appended_data1['caseid'] = df5_appended_data1['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df5_appended_data2['primaryid'] = df5_appended_data2['primaryid'].astype(str).replace('\\.0', '', regex=True)\n",
        "df5_appended_data2['caseid'] = df5_appended_data2['caseid'].astype(str).replace('\\.0', '', regex=True)\n",
        "\n",
        "df5_appended_data0['primaryid'] = df5_appended_data0['primaryid'].astype(str)\n",
        "df5_appended_data0['caseid'] = df5_appended_data0['caseid'].astype(str)\n",
        "\n",
        "df5_appended_data1['primaryid'] = df5_appended_data1['primaryid'].astype(str)\n",
        "df5_appended_data1['caseid'] = df5_appended_data1['caseid'].astype(str)\n",
        "\n",
        "df5_appended_data2['primaryid'] = df5_appended_data2['primaryid'].astype(str)\n",
        "df5_appended_data2['caseid'] = df5_appended_data2['caseid'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IECDAShIMdTV"
      },
      "source": [
        "\n",
        "df2_appended_data0['drugname']= df2_appended_data0['drugname'].str.lower()\n",
        "df2_appended_data0b['drugname']= df2_appended_data0b['drugname'].str.lower()\n",
        "df2_appended_data0c['drugname']= df2_appended_data0c['drugname'].str.lower()\n",
        "\n",
        "df2_appended_data1a['drugname']= df2_appended_data1a['drugname'].str.lower()\n",
        "df2_appended_data1b['drugname']= df2_appended_data1b['drugname'].str.lower()\n",
        "df2_appended_data1c['drugname']= df2_appended_data1c['drugname'].str.lower()\n",
        "\n",
        "df2_appended_data2a['drugname']= df2_appended_data2a['drugname'].str.lower()\n",
        "df2_appended_data2b['drugname']= df2_appended_data2b['drugname'].str.lower()\n",
        "\n",
        "#############################\n",
        "\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.strip()\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.strip(',')\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.replace('\\n',\"\")\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data0['drugname'] = [''.join(c.split()) for c in df2_appended_data0['drugname'].astype(str)]\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data0['drugname'] = df2_appended_data0['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "########################\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.strip()\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.strip(',')\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.replace('\\n',\"\")\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data0b['drugname'] = [''.join(c.split()) for c in df2_appended_data0b['drugname'].astype(str)]\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data0b['drugname'] = df2_appended_data0b['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "#############################\n",
        "\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.strip()\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.strip(',')\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.replace('\\n',\"\")\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data0c['drugname'] = [''.join(c.split()) for c in df2_appended_data0c['drugname'].astype(str)]\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data0c['drugname'] = df2_appended_data0c['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "####################\n",
        "\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.strip()\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.strip(',')\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.replace(' \\n',\"\")\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data1a['drugname'] = [''.join(c.split()) for c in df2_appended_data1a['drugname'].astype(str)]\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data1a['drugname'] = df2_appended_data1a['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "####################\n",
        "\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.strip()\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.strip(',')\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.replace(' \\n',\"\")\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data1b['drugname'] = [''.join(c.split()) for c in df2_appended_data1b['drugname'].astype(str)]\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data1b['drugname'] = df2_appended_data1b['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "####################\n",
        "\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.strip()\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.strip(',')\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.replace(' \\n',\"\")\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data1c['drugname'] = [''.join(c.split()) for c in df2_appended_data1c['drugname'].astype(str)]\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data1c['drugname'] = df2_appended_data1c['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "########################\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.strip()\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.strip(',')\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.replace(' \\n',\"\")\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data2a['drugname'] = [''.join(c.split()) for c in df2_appended_data2a['drugname'].astype(str)]\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data2a['drugname'] = df2_appended_data2a['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n",
        "\n",
        "########################\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.strip()\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.strip(',')\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.replace(u\"/\",\", \")\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.replace(u\"\\\\\\\\\", \", \")\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.replace(u\"' \",\"'\")\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.replace('.',\"\")\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.replace(' \\n',\"\")\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.replace('\\n ',\"\")\n",
        "df2_appended_data2b['drugname'] = [''.join(c.split()) for c in df2_appended_data2b['drugname'].astype(str)]\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r\"\\n\",  ' ', regex=True)\n",
        "df2_appended_data2b['drugname'] = df2_appended_data2b['drugname'].str.translate({None: \",!.; ''-@!%^&*)(\"})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFppkXvxkgJ_"
      },
      "source": [
        "#os.chdir('/content/drive/MyDrive/colab_faers_data/Data/helper_dataset')\n",
        "##country_file_url = 'https://opendata.arcgis.com/datasets/2b93b06dc0dc4e809d3c8db5cb96ba69_0.zip'\n",
        "#file_name = 'world'+'.zip'\n",
        "\n",
        "#with urllib.request.urlopen(country_file_url) as response, open(file_name, 'wb') as out_file:\n",
        " # shutil.copyfileobj(response, out_file)\n",
        "\n",
        "#print('zipfile download complete')\n",
        "\n",
        "#file_name = 'world'+'.zip'\n",
        "#print(file_name)\n",
        "\n",
        "#os.chdir('/content/drive/MyDrive/colab_faers_data/Data/helper_dataset')\n",
        "#import zipfile\n",
        "#zip_ref = zipfile.ZipFile(file_name)\n",
        "#zip_ref.printdir() \n",
        "#os.chdir('/content/drive/MyDrive/colab_faers_data/Data/helper_dataset')\n",
        "#zip_ref.extractall('world')\n",
        "#zip_ref.close()\n",
        "#print('zipfile unzip complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DndoCH3fMdTX"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/colab_faers_data/Data/helper_dataset/world')\n",
        "\n",
        "world_shp = gpd.read_file(filename='World_Countries__Generalized_.shp')\n",
        "world_shp['ISO'] = world_shp['ISO'].str.strip()#ISOSHRTNAM\n",
        "world_shp['ISO'] = world_shp['ISO'].str.lower()\n",
        "europe = ['AL', 'AD', 'AM', 'AT', 'BY', 'BE', 'BA', 'BG', 'CH', 'CY', 'CZ', 'DE',\n",
        "        'DK', 'EE', 'ES', 'FO', 'FI', 'FR', 'GB', 'GE', 'GI', 'GR', 'HU', 'HR',\n",
        "        'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'MC', 'MK', 'MT', 'NO', 'NL', 'PL',\n",
        "        'PT', 'RO', 'SE', 'SI', 'SK', 'SM', 'TR', 'UA', 'VA']\n",
        "world_shp = world_shp[world_shp['ISO'].str.contains('|'.join(europe),case=False, na=False, regex=True)]\n",
        "world_shp['centroid'] = world_shp['geometry'].centroid\n",
        "world_shp['coords'] = world_shp['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
        "world_shp['coords'] = [coords[0] for coords in world_shp['coords']]\n",
        "world_shp['coords']\n",
        "\n",
        "world_shp['Lon'] = world_shp.centroid.x\n",
        "world_shp['Lat'] = world_shp.centroid.y\n",
        "\n",
        "world_shp = world_shp.reset_index()\n",
        "world_shp.columns\n",
        "#world_shp.head()\n",
        "world_shp[['COUNTRY','ISO','geometry','coords']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwucfj5MdTY"
      },
      "source": [
        "df1_appended_data0['occr_country'] = df1_appended_data0['occr_country'].str.strip()\n",
        "df1_appended_data0['occr_country'] = df1_appended_data0['occr_country'].str.lower()\n",
        "\n",
        "df_e0 =df1_appended_data0.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUAZsqUUndIO"
      },
      "source": [
        "df1_appended_data0b['occr_country'] = df1_appended_data0b['occr_country'].str.strip()\n",
        "df1_appended_data0b['occr_country'] = df1_appended_data0b['occr_country'].str.lower()\n",
        "\n",
        "df_e0b =df1_appended_data0b.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnVoZBtlRtzj"
      },
      "source": [
        "df1_appended_data0c['occr_country'] = df1_appended_data0c['occr_country'].str.strip()\n",
        "df1_appended_data0c['occr_country'] = df1_appended_data0c['occr_country'].str.lower()\n",
        "\n",
        "df_e0c =df1_appended_data0.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF-C_4s3MdTZ"
      },
      "source": [
        "df1_appended_data1a['occr_country'] = df1_appended_data1a['occr_country'].str.strip()\n",
        "df1_appended_data1a['occr_country'] = df1_appended_data1a['occr_country'].str.lower()\n",
        "\n",
        "df_e1a =df1_appended_data1a.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8ww4ir4RtpL"
      },
      "source": [
        "df1_appended_data1b['occr_country'] = df1_appended_data1b['occr_country'].str.strip()\n",
        "df1_appended_data1b['occr_country'] = df1_appended_data1b['occr_country'].str.lower()\n",
        "\n",
        "df_e1b =df1_appended_data1b.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Im1_XUTndTr"
      },
      "source": [
        "df1_appended_data1c['occr_country'] = df1_appended_data1c['occr_country'].str.strip()\n",
        "df1_appended_data1c['occr_country'] = df1_appended_data1c['occr_country'].str.lower()\n",
        "\n",
        "df_e1c =df1_appended_data1c.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MckEytWPMdTa"
      },
      "source": [
        "df1_appended_data2a['occr_country'] = df1_appended_data2a['occr_country'].str.strip()\n",
        "df1_appended_data2a['occr_country'] = df1_appended_data2a['occr_country'].str.lower()\n",
        "\n",
        "df_e2a =df1_appended_data2a.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_FDzK_vS1tA"
      },
      "source": [
        "df1_appended_data2b['occr_country'] = df1_appended_data2b['occr_country'].str.strip()\n",
        "df1_appended_data2b['occr_country'] = df1_appended_data2b['occr_country'].str.lower()\n",
        "\n",
        "df_e2b =df1_appended_data2b.merge(world_shp[['COUNTRY','ISO','geometry','coords']], left_on='occr_country', right_on='ISO', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTCCinviMdTb"
      },
      "source": [
        "import gc\n",
        "collected = gc.collect()\n",
        "gc.collect()\n",
        "\n",
        "print (\"Garbage collector: collected %d objects.\" % (collected))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcEIogx7MdTb"
      },
      "source": [
        "data_frames0 = [df_e0, df2_appended_data0,df3_appended_data0,df5_appended_data0]\n",
        "df_merged0 = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid','caseid'],\n",
        "                                            how='inner'), data_frames0)\n",
        "\n",
        "#df_merged0.head()\n",
        "#dd = df_merged0[['caseid_x', 'caseid_y','caseid']]\n",
        "#duplicateRowsDF1 = dd.drop_duplicates(subset=['caseid_x', 'caseid_y','caseid'], keep='last')\n",
        "#duplicateRowsDF = duplicateRowsDF1.drop_duplicates(subset=['caseid_x', 'caseid_y','caseid'], keep=False)\n",
        "#duplicateRowsDF\n",
        "#duplicateRowsDF1 = duplicateRowsDF1.T.drop_duplicates().T\n",
        "#duplicateRowsDF1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuBNNdp2TmJ_"
      },
      "source": [
        "#del(df_e0, df2_appended_data0,df3_appended_data0,df5_appended_data0,data_frames0) \n",
        "df_merged0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whooEIHyU85D"
      },
      "source": [
        "#pd.Dataframe.to_csv('/content/drive/MyDrive/colab_faers_data/Data/temp_data/df_merged0.csv')\n",
        "#df_merged0.to_csv('/content/drive/MyDrive/colab_faers_data/Data/temp_data/df_merged0.csv')\n",
        "df_merged0.to_pickle('/content/drive/MyDrive/colab_faers_data/Data/temp_data/df_merged0.parquet.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaIWdWMprvLb"
      },
      "source": [
        "stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6W2m1UdTPJG"
      },
      "source": [
        "data_frames0b = [df_e0b, df2_appended_data0b,df3_appended_data0b,df5_appended_data0b]\n",
        "df_merged0b = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames0b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrx1dnITUlPc"
      },
      "source": [
        "del(df_e0b, df2_appended_data0b,df3_appended_data0b,df5_appended_data0b,data_frames0b) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sczrADAtTPZW"
      },
      "source": [
        "data_frames0c = [df_e0c, df2_appended_data0c,df3_appended_data0c,df5_appended_data0c]\n",
        "df_merged0c = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames0c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq_RJ3Stn43g"
      },
      "source": [
        "data_frames0b = [df_e0b, df2_appended_data0b,df3_appended_data0b,df5_appended_data0b]\n",
        "df_merged0b = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames0b)\n",
        "\n",
        "df_merged0b.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqrZmKgn5EQ"
      },
      "source": [
        "data_frames0c = [df_e0c, df2_appended_data0c,df3_appended_data0c,df5_appended_data0c]\n",
        "df_merged0c = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames0c)\n",
        "\n",
        "df_merged0c.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX_mMBgmMdTd"
      },
      "source": [
        "data_frames1 = [df_e1, df2_appended_data1,df3_appended_data1,df5_appended_data1]\n",
        "\n",
        "\n",
        "df_merged1 = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames1)\n",
        "\n",
        "df_merged1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DModAlirMdTe"
      },
      "source": [
        "data_frames1b = [df_e1b, df2_appended_data1b,df5_appended_data1b]\n",
        "\n",
        "\n",
        "df_merged1b = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames1b)\n",
        "\n",
        "df_merged1b.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uciZZcNAMdTe"
      },
      "source": [
        "data_frames2 = [df_e2, df2_appended_data2,df3_appended_data2,df5_appended_data2]\n",
        "df_merged2 = reduce(lambda  left,right: pd.merge(left,right,on=['primaryid'],\n",
        "                                            how='inner'), data_frames2)\n",
        "\n",
        "df_merged2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcDC14D4MdTf"
      },
      "source": [
        "df_merged0.filter(['primaryid','caseid','lot_num', 'nda_num','pt','drugname','prod_ai','reporter_country','occr_country','event_dt'])\n",
        "\n",
        "df_merged0.groupby(['reporter_country']).ngroups\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HDDmlhYMdTg"
      },
      "source": [
        "df_merged1.filter(['primaryid','caseid','lot_num', 'nda_num','pt','drugname','prod_ai','reporter_country','occr_country','event_dt'])\n",
        "\n",
        "df_merged1.groupby(['reporter_country']).ngroups\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlTN7pd4MdTh"
      },
      "source": [
        "import gc\n",
        "collected = gc.collect()\n",
        "gc.collect()\n",
        "\n",
        "print (\"Garbage collector: collected %d objects.\" % (collected))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76MTZtITMdTi"
      },
      "source": [
        "df_merged2.filter(['primaryid','caseid','lot_num', 'nda_num','pt','drugname','prod_ai','reporter_country','occr_country','event_dt'])\n",
        "\n",
        "df_merged2.groupby(['reporter_country']).ngroups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q18Vo4voMdTi"
      },
      "source": [
        "df_merged0['event_dt'] = df_merged0['event_dt'].astype(str).replace('\\.0', '', regex=True)\n",
        "df_merged0['event_dt'] = df_merged0['event_dt'].astype(str)\n",
        "df_merged0['fda_dt'] = df_merged0['fda_dt'].astype(str).replace('\\.0', '', regex=True)\n",
        "df_merged0['fda_dt'] = df_merged0['fda_dt'].astype(str)\n",
        "df_merged0.fillna(0)\n",
        "df_merged0.replace(np.nan, 0, inplace=True)\n",
        "\n",
        "df_merged0['event_dt'] = pd.to_datetime(df_merged0.event_dt, errors='coerce')\n",
        "df_merged0['event_dt1'] = df_merged0['event_dt'].dt.strftime('%Y%m%d')\n",
        "df_merged0['fda_dt'] = pd.to_datetime(df_merged0.fda_dt, errors='coerce')\n",
        "df_merged0['fda_dt1'] = df_merged0['fda_dt'].dt.strftime('%Y%m%d')\n",
        "\n",
        "df_merged0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biPQjYktMdTj"
      },
      "source": [
        "df_merged1['event_dt'] = df_merged1['event_dt'].astype(str).replace('\\.0', '', regex=True)\n",
        "df_merged1['event_dt'] = df_merged1['event_dt'].astype(str)\n",
        "df_merged1['fda_dt'] = df_merged1['fda_dt'].astype(str).replace('\\.0', '', regex=True)\n",
        "df_merged1['fda_dt'] = df_merged1['fda_dt'].astype(str)\n",
        "df_merged1.fillna(0)\n",
        "df_merged1.replace(np.nan, 0, inplace=True)\n",
        "\n",
        "df_merged1['event_dt'] = pd.to_datetime(df_merged1.event_dt, errors='coerce')\n",
        "df_merged1['event_dt1'] = df_merged1['event_dt'].dt.strftime('%Y%m%d')\n",
        "df_merged1['fda_dt'] = pd.to_datetime(df_merged1.fda_dt, errors='coerce')\n",
        "df_merged1['fda_dt1'] = df_merged1['fda_dt'].dt.strftime('%Y%m%d')\n",
        "\n",
        "df_merged1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DjSr4FVMdTj"
      },
      "source": [
        "df_merged2['event_dt'] = df_merged2['event_dt'].astype(str).replace('\\.0', '', regex=True)\n",
        "df_merged2['event_dt'] = df_merged2['event_dt'].astype(str)\n",
        "df_merged2['fda_dt'] = df_merged2['fda_dt'].astype(str).replace('\\.0', '', regex=True)\n",
        "df_merged2['fda_dt'] = df_merged2['fda_dt'].astype(str)\n",
        "\n",
        "df_merged2.fillna(0)\n",
        "df_merged2.replace(np.nan, 0, inplace=True)\n",
        "\n",
        "df_merged2['event_dt'] = pd.to_datetime(df_merged2.event_dt, errors='coerce')\n",
        "df_merged2['event_dt1'] = df_merged2['event_dt'].dt.strftime('%Y%m%d')\n",
        "df_merged2['fda_dt'] = pd.to_datetime(df_merged2.fda_dt, errors='coerce')\n",
        "df_merged2['fda_dt1'] = df_merged2['fda_dt'].dt.strftime('%Y%m%d')\n",
        "df_merged2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cl0IEBXMdTk"
      },
      "source": [
        "os.chdir(r\"\\\\Fdswp03752\\ocmgis_data\\Scripts\\Situation_Awareness_Natural_hazards\\shapefiles\\World\")\n",
        "\n",
        "world_shp = gpd.read_file(filename='world.shp')\n",
        "world_shp['ISO_2DIGIT'] = world_shp['ISO_2DIGIT'].str.strip()#ISOSHRTNAM\n",
        "world_shp['ISO_2DIGIT'] = world_shp['ISO_2DIGIT'].str.lower()\n",
        "europe = ['AL', 'AD', 'AM', 'AT', 'BY', 'BE', 'BA', 'BG', 'CH', 'CY', 'CZ', 'DE',\n",
        "        'DK', 'EE', 'ES', 'FO', 'FI', 'FR', 'GB', 'GE', 'GI', 'GR', 'HU', 'HR',\n",
        "        'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'MC', 'MK', 'MT', 'NO', 'NL', 'PL',\n",
        "        'PT', 'RO', 'SE', 'SI', 'SK', 'SM', 'TR', 'UA', 'VA']\n",
        "world_shp = world_shp[world_shp['ISO_2DIGIT'].str.contains('|'.join(europe),case=False, na=False, regex=True)]\n",
        "world_shp['centroid'] = world_shp['geometry'].centroid\n",
        "world_shp['coords'] = world_shp['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
        "world_shp['coords'] = [coords[0] for coords in world_shp['coords']]\n",
        "world_shp['coords']\n",
        "\n",
        "world_shp['Lon'] = world_shp.centroid.x\n",
        "world_shp['Lat'] = world_shp.centroid.y\n",
        "\n",
        "world_shp = world_shp.reset_index()\n",
        "world_shp.columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv--xAERMdTk"
      },
      "source": [
        "df_merged0['occr_country'] = df_merged0['occr_country'].str.strip()\n",
        "df_merged0['occr_country'] = df_merged0['occr_country'].str.lower()\n",
        "\n",
        "#df_0 =df_merged0.merge(world_shp, left_on='occr_country', right_on='ISO_2DIGIT', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABuPM9zcMdTl"
      },
      "source": [
        "df_merged1['occr_country'] = df_merged1['occr_country'].str.strip()\n",
        "df_merged1['occr_country'] = df_merged1['occr_country'].str.lower()\n",
        "\n",
        "#df_1 =df_merged1.merge(world_shp, left_on='occr_country', right_on='ISO_2DIGIT', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p1HpLI1MdTl"
      },
      "source": [
        "df_merged2['occr_country'] = df_merged2['occr_country'].str.strip()\n",
        "df_merged2['occr_country'] = df_merged2['occr_country'].str.lower()\n",
        "\n",
        "#df_2 =df_merged2.merge(world_shp, left_on='occr_country', right_on='ISO_2DIGIT', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR8OV1WLMdTm"
      },
      "source": [
        "df_merged0_=df_merged0\n",
        "df_merged1_=df_merged1\n",
        "df_merged2_=df_merged2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tns-4UfcMdTn"
      },
      "source": [
        "\n",
        "df_merged0=df_merged0_\n",
        "df_merged1=df_merged1_\n",
        "df_merged2=df_merged2_\n",
        "\n",
        "frames = [df_merged0,df_merged1,df_merged2]\n",
        "\n",
        "df_joined_drug = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up8C152jMdTn"
      },
      "source": [
        "df_joined_drug=df_joined_drug['drugname'].groupby([df_joined_drug.primaryid,df_joined_drug.caseid,df_joined_drug.lot_num,df_joined_drug.nda_num, df_joined_drug.event_dt, \n",
        "                                                           df_joined_drug.occr_country, df_joined_drug.pt,\n",
        "                        df_joined_drug.reporter_country,df_joined_drug.CNTRY_NAME,df_joined_drug.fda_dt]).apply(list).reset_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4UQsz-cMdTo"
      },
      "source": [
        "#len(df_1.pt.iloc[5])\n",
        "#df_1.pt.iloc[5]\n",
        "\n",
        "df_joined_drug[\"count_drugs\"] = np.nan\n",
        "lenghts = []\n",
        "for index, row in df_joined_drug.iterrows():\n",
        "        #print(row[[\"geometry\",\"GaugeLID\"]])\n",
        "\n",
        "    length = len(df_joined_drug.drugname.iloc[index])\n",
        "    lenghts.append(length)\n",
        "df_joined_drug['count_drugs'] = lenghts   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIw-xhvSMdTp"
      },
      "source": [
        "df_joined_drug_=df_joined_drug[df_joined_drug['count_drugs']==1]\n",
        "df_joined_drug_['drugname'] = df_joined_drug_['drugname'].astype(str)\n",
        "\n",
        "df_joined_drug_['drugname'] = df_joined_drug_['drugname'].str.lstrip('[')\n",
        "df_joined_drug_['drugname'] = df_joined_drug_['drugname'].str.rstrip(']')\n",
        "df_joined_drug_['drugname'] = df_joined_drug_['drugname'].str.lstrip(\"'\")\n",
        "df_joined_drug_['drugname'] = df_joined_drug_['drugname'].str.rstrip(\"'\")\n",
        "\n",
        "df_joined_drug_.info()\n",
        "#df_joined_drug_.drugname.value_counts()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVdep1TJMdTq"
      },
      "source": [
        "df_joined_drug_.drugname.value_counts().head(20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUZk4SoCMdTr"
      },
      "source": [
        "stop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwAj7MlMdTs"
      },
      "source": [
        "df_= df_joined_drug_[df_joined_drug_['drugname'].isin(['nivolumab'])]\n",
        "#df_joined_drug_.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HALBn6ijMdTs"
      },
      "source": [
        "#df_joined_drug.drugname.value_counts()\n",
        "df_ = df_.filter(['primaryid','caseid','lot_num', 'nda_num','pt','drugname','prod_ai','reporter_country','occr_country','event_dt','CNTRY_NAME','fda_dt', 'geometry'])\n",
        "\n",
        "df_.CNTRY_NAME.value_counts().head(20)\n",
        "#df_.prod_ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oUuvDMvMdTt"
      },
      "source": [
        "total_rows = df_['primaryid'].count()\n",
        "#print(total_rows2+1)\n",
        "print('total rows: ', total_rows+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c21wj5_aMdTt"
      },
      "source": [
        "total_rows = df_['primaryid'].count()\n",
        "print('total rows: ',total_rows+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShX3feSpMdTu"
      },
      "source": [
        "plt.style.use(['dark_background'])\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(num=None, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.rc('xtick', labelsize=20) \n",
        "plt.rc('ytick', labelsize=20) \n",
        "df_['event_dt'].groupby(df_[\"event_dt\"].dt.year).count().plot(kind=\"bar\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys9SknHrMdTu"
      },
      "source": [
        "#df_natalizumab.drop(['fda_dt'], axis=1,inplace=True)\n",
        "df_['one'] = int(1)\n",
        "df_['year'] = pd.DatetimeIndex(df_['event_dt']).year\n",
        "df_['quarter'] = pd.DatetimeIndex(df_['event_dt']).quarter\n",
        "\n",
        "df_.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOwslOiVMdTu"
      },
      "source": [
        "os.chdir(r\"D:\\Research\\FAERS_Public_LDA\\Data\\Analysis_Ready_data\\Extracted_data\")\n",
        "#df_.to_csv('df_enbrel_europe.csv')\n",
        "#df_.to_csv('df_remicade_europe.csv')\n",
        "#df_.to_csv('df_humira_europe.csv')\n",
        "#df_.to_csv('df_dexamethasone_europe.csv')\n",
        "df_.to_csv('df_nivolumab_europe.csv')\n",
        "#xarelto\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh73fABTMdTv"
      },
      "source": [
        "df_.primaryid.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJuGP60RMdTv"
      },
      "source": [
        "df_=df_[df_.occr_country.str.contains('de|se|si',na=False, case=False)]\n",
        "df_= df_[df_.pt.str.contains('Fall',na=False, regex=True, case=False)]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPk6dIruMdTv"
      },
      "source": [
        "df_.head()\n",
        "df_1=df_['pt'].groupby([df_.primaryid,df_.caseid,df_.lot_num,df_.nda_num, df_.event_dt, df_.occr_country, df_.drugname,\n",
        "                        df_.reporter_country,df_.CNTRY_NAME,df_.fda_dt,\n",
        "                        df_.one,df_.year,df_.quarter]).apply(list).reset_index()\n",
        "\n",
        "\n",
        "\n",
        "df_1[\"count_pt\"] = np.nan\n",
        "lenghts = []\n",
        "for index, row in df_1.iterrows():\n",
        "        #print(row[[\"geometry\",\"GaugeLID\"]])\n",
        "\n",
        "    length = len(df_1.pt.iloc[index])\n",
        "    lenghts.append(length)\n",
        "df_1['count_pt'] = lenghts   \n",
        "\n",
        "df_1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyhKhgREMdTw"
      },
      "source": [
        "df_.head()\n",
        "df_.pt = df_.pt.astype(str)\n",
        "df_['pt'] = df_['pt'].str.strip()\n",
        "df_['pt'] = df_['pt'].str.strip(',')\n",
        "df_['pt']=df_['pt'].str.lower()\n",
        "\n",
        "df_1=df_['occr_country'].groupby([df_.pt,df_.drugname,df_.event_dt,df_.year,df_.quarter]).apply(list).reset_index()\n",
        "\n",
        "\n",
        "df_1[\"count_occr_country\"] = np.nan\n",
        "lenghts = []\n",
        "for index, row in df_1.iterrows():\n",
        "        #print(row[[\"geometry\",\"GaugeLID\"]])\n",
        "\n",
        "    length = len(df_1.occr_country.iloc[index])\n",
        "    lenghts.append(length)\n",
        "df_1['count_occr_country'] = lenghts  \n",
        "#df_1['occr_country'] = df_1['occr_country'].str.split(',').apply(set).str.join(',')\n",
        "\n",
        "\n",
        "df_1.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gRWnxjmMdTw"
      },
      "source": [
        "df_1[df_1['count_occr_country']>=2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHZgNsyMdTw"
      },
      "source": [
        "#df_1[df_1.pt=='fall']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwO8m97-MdTx"
      },
      "source": [
        "df_1.primaryid = df_1.primaryid.astype(str)\n",
        "pid= '100041803'\n",
        "df_1_=df_1[df_1.primaryid.str.contains(pid)]\n",
        "df_1_.pt.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McYWlYvlMdTx"
      },
      "source": [
        "#os.chdir(r\"G:\\Research\\FAERS_Public\\Data\\Analysis_Ready_data\\Extracted_data\")\n",
        "#df_1.to_csv('df_duodopa_europe_PT_joined.csv')\n",
        "df_1.to_csv('df_nivolumab_europe_PT_joined.csv')\n",
        "#df_1.to_csv('df_dexamethasone_europe_PT_joined.csv')\n",
        "#df_1.to_csv('df_omeprazole_europe_PT_joined.csv')\n",
        "#df_1.to_csv('df_remicade_europe_occr_country_joined.csv')\n",
        "#df_1.to_csv('df_methotrexate_europe_PT_joined.csv')\n",
        "#df_1.to_csv('df_remicade_europe_PT_joined.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f7HQp1BMdTy"
      },
      "source": [
        "df_1[df_1.occr_country=='dk']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxbDmepBMdTz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}